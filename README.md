#  Multi-Modal Assistive System for the Visually Impaired

Visually impaired individuals face challenges in navigating and understanding their surroundings. This project introduces a Multi-Modal Assistive System using LLaMA 3.2 Vision, gesture-based image capture, and voice-based querying for real-time AI-driven descriptions. With location tracking and emergency alerts, it enhances accessibility, safety, and independence.

#  The primary objectives of this system are:
.Real-Time Environmental Awareness – Utilize LLaMA 3.2 Vision to generate automatic scene descriptions, enabling visually impaired users to understand their surroundings.
Accurate and Contextual Information – Identify and describe objects, people, and text in real-world scenarios with high precision.
Enhanced Independence & Safety – Provide continuous visual assistance to support users in daily activities while integrating location tracking and emergency alerts.
 Intuitive Interaction – Implement gesture-based image capture and voice-controlled image querying for seamless usability.

